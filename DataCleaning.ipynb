{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "821c8a93",
   "metadata": {},
   "source": [
    "   # Data Cleaning\n",
    "   ## The following codes were used to clean and understand or \"visualize\" our data before using ParaView."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72052851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from geopy.geocoders import Photon\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a3b91",
   "metadata": {},
   "source": [
    "## Task List\n",
    "\n",
    "- CLEAN THE LOCATION DATA\n",
    "- GET THE LAT LONG DATA FOR EACH LOCATION\n",
    "- GET THE UNIQUE TOXINS\n",
    "- GET THE UNIQUE MEASUREMENT VALUES PER TOXIN\n",
    "- GET THE TIME PERIOD (TO BE USED FOR VISUALIZATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d19e70",
   "metadata": {},
   "source": [
    "### Clean the location data\n",
    "\n",
    "Some rows contained data for multiple locations in one row (i.e. the location is \"Long Island City - Astoria\" or \"Long Island City and Astoria\").  To accurately plot a toxin reading for a single location, rows containing locations like this were split and the data was duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b3eb71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unique ID  Indicator ID                    Name Measure Measure Info  \\\n",
      "0     172653           375  Nitrogen dioxide (NO2)    Mean          ppb   \n",
      "1     172585           375  Nitrogen dioxide (NO2)    Mean          ppb   \n",
      "2     336637           375  Nitrogen dioxide (NO2)    Mean          ppb   \n",
      "3     336622           375  Nitrogen dioxide (NO2)    Mean          ppb   \n",
      "4     172582           375  Nitrogen dioxide (NO2)    Mean          ppb   \n",
      "\n",
      "  Geo Type Name  Geo Join ID                      Geo Place Name  \\\n",
      "0         UHF34          203  Bedford Stuyvesant - Crown Heights   \n",
      "1         UHF34          203  Bedford Stuyvesant - Crown Heights   \n",
      "2         UHF34          204                       East New York   \n",
      "3         UHF34          103                  Fordham - Bronx Pk   \n",
      "4         UHF34          104                Pelham - Throgs Neck   \n",
      "\n",
      "           Time Period  Start_Date  Data Value  Message  \n",
      "0  Annual Average 2011  12/01/2010       25.30      NaN  \n",
      "1  Annual Average 2009  12/01/2008       26.93      NaN  \n",
      "2  Annual Average 2015  01/01/2015       19.09      NaN  \n",
      "3  Annual Average 2015  01/01/2015       19.76      NaN  \n",
      "4  Annual Average 2009  12/01/2008       22.83      NaN  \n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('Air_Quality_20231105.csv')\n",
    "\n",
    "# Print the first 5 rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95da0652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique ID\n",
      "Indicator ID\n",
      "Name\n",
      "Measure\n",
      "Measure Info\n",
      "Geo Type Name\n",
      "Geo Join ID\n",
      "Geo Place Name\n",
      "Time Period\n",
      "Start_Date\n",
      "Data Value\n",
      "Message\n"
     ]
    }
   ],
   "source": [
    "# Print all column names\n",
    "for column in df.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51398369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns where all values are NaN\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv('Air_Quality_V1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3895f3fd",
   "metadata": {},
   "source": [
    "There are codes in parenthesis attached to the locations. The code below removes them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b3c5573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_content_within_parenthesis(content):\n",
    "    # This regex pattern will find and remove any content within parentheses, including the parentheses themselves\n",
    "    return re.sub(r'\\(.*?\\)', '', content).strip()\n",
    "\n",
    "def main():\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(\"Air_Quality_V1.csv\")\n",
    "\n",
    "    # Clean the 'Geo Place Name' column\n",
    "    data['Geo Place Name'] = data['Geo Place Name'].apply(lambda x: remove_content_within_parenthesis(str(x)))\n",
    "    \n",
    "    # Save the updated CSV\n",
    "    data.to_csv(\"Air_Quality_V2.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1dc373",
   "metadata": {},
   "source": [
    "Some of the toxin readings are for multiple locations as indicated by the word 'and' in the name. Move those locations to a different spreadsheet, remove the 'and', duplicate the data for those rows, and merge the spreadsheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c337952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the existing CSV file\n",
    "df = pd.read_csv('Air_Quality_V2.csv')\n",
    "\n",
    "# Filter rows where 'Geo Place Name' contains 'and'\n",
    "matching_rows = df[df['Geo Place Name'].str.contains(' and ', case=False, na=False)]\n",
    "\n",
    "# Filter rows where 'Geo Place Name' does NOT contain 'and'\n",
    "non_matching_rows = df[~df['Geo Place Name'].str.contains(' and ', case=False, na=False)]\n",
    "\n",
    "# Write these DataFrames to new CSV files\n",
    "matching_rows.to_csv('Air_Quality_V2_AND.csv', index=False)\n",
    "non_matching_rows.to_csv('Air_Quality_V2_REMAINING.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4abaeeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roziena Badree\\AppData\\Local\\Temp\\ipykernel_8576\\2421511114.py:21: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_rows = new_rows.append(new_row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Read the existing CSV file\n",
    "df = pd.read_csv('Air_Quality_V2_AND.csv')\n",
    "\n",
    "# Create an empty DataFrame to store rows with split 'Geo Place Name'\n",
    "new_rows = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "# Iterate over each row in the original DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    if ' and ' in str(row['Geo Place Name']):\n",
    "        # Split the 'Geo Place Name' value\n",
    "        first_location, second_location = row['Geo Place Name'].split(' and ', 1)\n",
    "        \n",
    "        # Update the original row's 'Geo Place Name' value with the first location\n",
    "        df.at[index, 'Geo Place Name'] = first_location\n",
    "        \n",
    "        # Copy the row and update the 'Geo Place Name' with the second location\n",
    "        new_row = row.copy()\n",
    "        new_row['Geo Place Name'] = second_location\n",
    "        \n",
    "        # Append the new row to the new_rows DataFrame\n",
    "        new_rows = new_rows.append(new_row, ignore_index=True)\n",
    "\n",
    "# Concatenate the original DataFrame and the new_rows DataFrame\n",
    "df = pd.concat([df, new_rows], ignore_index=True)\n",
    "\n",
    "# Write the updated DataFrame to a new CSV file\n",
    "df.to_csv('Air_Quality_V2_AND_V2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce2c3944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roziena Badree\\AppData\\Local\\Temp\\ipykernel_8576\\3423243174.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  combined_df = df1.append(df2, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Read both CSV files\n",
    "df1 = pd.read_csv('Air_Quality_V2_REMAINING.csv')\n",
    "df2 = pd.read_csv('Air_Quality_V2_AND_V2.csv')\n",
    "\n",
    "# Append the data from the second DataFrame to the first\n",
    "combined_df = df1.append(df2, ignore_index=True)\n",
    "\n",
    "# Write the combined data to a new CSV file\n",
    "combined_df.to_csv('Air_Quality_V3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca19f4f",
   "metadata": {},
   "source": [
    "Some of the toxin readings are for multiple locations as indicated by the '-' in the name. Move those locations to a different spreadsheet, remove the '-', duplicate the data for those rows, and merge the spreadsheets. Note that there is a city named Co-op City that does have a hyphen in the name. First, move that to a different spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e9943fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('Air_Quality_V3.csv')\n",
    "\n",
    "# Filter rows where 'Geo Place Name' is 'Co-op City'\n",
    "coop_city_df = df[df['Geo Place Name'] == 'Co-op City']\n",
    "\n",
    "# Write these rows to a new CSV file\n",
    "coop_city_df.to_csv('coop_city_rows.csv', index=False)\n",
    "\n",
    "# Filter out rows where 'Geo Place Name' is 'Co-op City' from the original DataFrame\n",
    "filtered_df = df[df['Geo Place Name'] != 'Co-op City']\n",
    "\n",
    "# Write the filtered data back to the original CSV file\n",
    "filtered_df.to_csv('Air_Quality_V3_V2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daa71861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('Air_Quality_V3_V2.csv')\n",
    "\n",
    "# List to store the expanded rows\n",
    "expanded_rows = []\n",
    "\n",
    "# Iterate over each row in the original DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Check if 'Geo Place Name' contains a '-'\n",
    "    if '-' in str(row['Geo Place Name']):\n",
    "        # Split the 'Geo Place Name' value\n",
    "        locations = row['Geo Place Name'].split('-')\n",
    "        for location in locations:\n",
    "            # Copy the row and update the 'Geo Place Name' with the current location\n",
    "            new_row = row.copy()\n",
    "            new_row['Geo Place Name'] = location.strip()  # strip to remove leading/trailing spaces\n",
    "            expanded_rows.append(new_row)\n",
    "    else:\n",
    "        expanded_rows.append(row)\n",
    "\n",
    "# Convert the list of rows to a DataFrame\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Write the expanded DataFrame to a new CSV file\n",
    "expanded_df.to_csv('Air_Quality_V4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e1462d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roziena Badree\\AppData\\Local\\Temp\\ipykernel_8576\\201534228.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  combined_df = df1.append(df2, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Read both CSV files\n",
    "df1 = pd.read_csv('Air_Quality_V4.csv')\n",
    "df2 = pd.read_csv('coop_city_rows.csv')\n",
    "\n",
    "# Append the data from the second DataFrame to the first\n",
    "combined_df = df1.append(df2, ignore_index=True)\n",
    "\n",
    "# Write the combined data to a new CSV file\n",
    "combined_df.to_csv('Air_Quality_V4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0c2b0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Geo Place Name\n",
      "0    Bedford Stuyvesant\n",
      "1         Crown Heights\n",
      "2         East New York\n",
      "3               Fordham\n",
      "4              Bronx Pk\n",
      "..                  ...\n",
      "128           Woodhaven\n",
      "129    Lefferts Gardens\n",
      "130       Starrett City\n",
      "131             Maspeth\n",
      "132          Co-op City\n",
      "\n",
      "[133 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('Air_Quality_V4.csv')\n",
    "\n",
    "# Get the unique locations\n",
    "column_name = 'Geo Place Name'\n",
    "\n",
    "# Get unique values from the specified column\n",
    "unique_values = df[column_name].unique()\n",
    "\n",
    "# Convert the unique values into a new DataFrame\n",
    "unique_df = pd.DataFrame(unique_values, columns=[column_name])\n",
    "\n",
    "# Print the information\n",
    "print(unique_df)\n",
    "\n",
    "# Save the unique locations to a new CSV file\n",
    "unique_df.to_csv('unique_locations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6759dd3",
   "metadata": {},
   "source": [
    "There is some manual labor associated with cleaning the data. For each unique location, input the correct borough in the neighboring column, then write both location and borough back to the original CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e841761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Read both CSV files\n",
    "    data1 = pd.read_csv(\"unique_locations.csv\")\n",
    "    data2 = pd.read_csv(\"Air_Quality_V4.csv\")\n",
    "\n",
    "    # Merge data based on the \"name\" column, keeping all names from the second CSV \n",
    "    # and bringing in the corresponding values from the first CSV\n",
    "    merged_data = data2.merge(data1, on=\"Geo Place Name\", how=\"left\")\n",
    "\n",
    "    # If you want to keep only columns \"name\" and \"value\" from the merged data\n",
    "    # merged_data = merged_data[[\"name\", \"value\"]]\n",
    "\n",
    "    # Save the merged data to a new CSV file\n",
    "    merged_data.to_csv(\"Air_Quality_V5.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "801e67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "data = pd.read_csv(\"Air_Quality_V5.csv\")\n",
    "\n",
    "# Combine the two columns with a comma and add 'NY' at the end\n",
    "data['Location'] = data['Geo Place Name'] + ', ' + data['Borough'] + ', NY'\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "data.to_csv(\"Air_Quality_V6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcbc023",
   "metadata": {},
   "source": [
    "Finally, we can get the latitude and longitude values for each unique location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "404448af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Location\n",
      "0    Bedford Stuyvesant, Brooklyn, NY\n",
      "1         Crown Heights, Brooklyn, NY\n",
      "2         East New York, Brooklyn, NY\n",
      "3                  Fordham, Bronx, NY\n",
      "4                 Bronx Pk, Bronx, NY\n",
      "..                                ...\n",
      "128             Woodhaven, Queens, NY\n",
      "129    Lefferts Gardens, Brooklyn, NY\n",
      "130       Starrett City, Brooklyn, NY\n",
      "131               Maspeth, Queens, NY\n",
      "132             Co-op City, Bronx, NY\n",
      "\n",
      "[133 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv(\"Air_Quality_V6.csv\")\n",
    "\n",
    "# Get the unique locations\n",
    "column_name = 'Location'\n",
    "\n",
    "# Get unique values from the specified column\n",
    "unique_values = df[column_name].unique()\n",
    "\n",
    "# Convert the unique values into a new DataFrame\n",
    "unique_df = pd.DataFrame(unique_values, columns=[column_name])\n",
    "\n",
    "# Print the information\n",
    "print(unique_df)\n",
    "\n",
    "# Save the unique locations to a new CSV file\n",
    "unique_df.to_csv('unique_locations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40bac571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geocode_place(place_name, geolocator):\n",
    "    try:\n",
    "        location = geolocator.geocode(place_name, timeout=10)\n",
    "        if location:\n",
    "            return location.latitude, location.longitude\n",
    "        else:\n",
    "            return None, None\n",
    "    except GeocoderTimedOut:\n",
    "        return geocode_place(place_name, geolocator)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(\"unique_locations.csv\")\n",
    "\n",
    "    # Extract unique values from 'Geo Place Name' column\n",
    "    unique_values = data['Location'].dropna().unique()\n",
    "\n",
    "    # Convert to DataFrame for saving to CSV\n",
    "    unique_df = pd.DataFrame(unique_values, columns=['Location'])\n",
    "\n",
    "    # Create a geolocator object\n",
    "    geolocator = Photon(user_agent=\"geoapiExercises\")\n",
    "\n",
    "    # Get lat-long for each geo place name\n",
    "    unique_df[['Latitude', 'Longitude']] = unique_df['Location'].apply(lambda x: pd.Series(geocode_place(x, geolocator)))\n",
    "\n",
    "    # Save the updated CSV\n",
    "    unique_df.to_csv(\"unique_locations.csv\", index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3835db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Read both CSV files\n",
    "    data1 = pd.read_csv(\"unique_locations.csv\")\n",
    "    data2 = pd.read_csv(\"Air_Quality_V6.csv\")\n",
    "\n",
    "    # Merge data based on the \"name\" column, keeping all names from the second CSV \n",
    "    # and bringing in the corresponding values from the first CSV\n",
    "    merged_data = data2.merge(data1, on=\"Location\", how=\"left\")\n",
    "\n",
    "    # If you want to keep only columns \"name\" and \"value\" from the merged data\n",
    "    # merged_data = merged_data[[\"name\", \"value\"]]\n",
    "\n",
    "    # Save the merged data to a new CSV file\n",
    "    merged_data.to_csv(\"Air_Quality_V7.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c455584",
   "metadata": {},
   "source": [
    "### Get the unique measurement values per toxin\n",
    "\n",
    "The following code was used to pull the unique toxins and their measurement values. The goal was to ensure that our data is uniform (i.e. there is only one measurement value per toxin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65693af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('Air_Quality_V7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38fe453c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Name\n",
      "0                              Nitrogen dioxide (NO2)\n",
      "1                             Fine particles (PM 2.5)\n",
      "2                                          Ozone (O3)\n",
      "3     Asthma emergency department visits due to PM2.5\n",
      "4                       Annual vehicle miles traveled\n",
      "5                Asthma hospitalizations due to Ozone\n",
      "6   Respiratory hospitalizations due to PM2.5 (age...\n",
      "7               Boiler Emissions- Total SO2 Emissions\n",
      "8   Cardiovascular hospitalizations due to PM2.5 (...\n",
      "9             Boiler Emissions- Total PM2.5 Emissions\n",
      "10              Boiler Emissions- Total NOx Emissions\n",
      "11              Annual vehicle miles travelled (cars)\n",
      "12            Annual vehicle miles travelled (trucks)\n",
      "13        Cardiac and respiratory deaths due to Ozone\n",
      "14   Asthma emergency departments visits due to Ozone\n",
      "15                  Outdoor Air Toxics - Formaldehyde\n",
      "16                       Outdoor Air Toxics - Benzene\n",
      "17                                Deaths due to PM2.5\n"
     ]
    }
   ],
   "source": [
    "# Specify the column name you want to get unique values from\n",
    "column_name = 'Name'\n",
    "\n",
    "# Get unique values from the specified column\n",
    "unique_values = df[column_name].unique()\n",
    "\n",
    "# Convert the unique values into a new DataFrame\n",
    "unique_df = pd.DataFrame(unique_values, columns=[column_name])\n",
    "\n",
    "# Print the information\n",
    "print(unique_df)\n",
    "\n",
    "# Save the unique values to a new CSV file\n",
    "# unique_df.to_csv('unique_measure_values.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcb0e4e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              measure  \\\n",
      "0                       Annual vehicle miles traveled   \n",
      "1               Annual vehicle miles travelled (cars)   \n",
      "2             Annual vehicle miles travelled (trucks)   \n",
      "3     Asthma emergency department visits due to PM2.5   \n",
      "4    Asthma emergency departments visits due to Ozone   \n",
      "5                Asthma hospitalizations due to Ozone   \n",
      "6               Boiler Emissions- Total NOx Emissions   \n",
      "7             Boiler Emissions- Total PM2.5 Emissions   \n",
      "8               Boiler Emissions- Total SO2 Emissions   \n",
      "9         Cardiac and respiratory deaths due to Ozone   \n",
      "10  Cardiovascular hospitalizations due to PM2.5 (...   \n",
      "11                                Deaths due to PM2.5   \n",
      "12                            Fine particles (PM 2.5)   \n",
      "13                             Nitrogen dioxide (NO2)   \n",
      "14                       Outdoor Air Toxics - Benzene   \n",
      "15                  Outdoor Air Toxics - Formaldehyde   \n",
      "16                                         Ozone (O3)   \n",
      "17  Respiratory hospitalizations due to PM2.5 (age...   \n",
      "\n",
      "                                       unique measure  \n",
      "0                                       million miles  \n",
      "1                                       million miles  \n",
      "2                                       million miles  \n",
      "3   Estimated annual rate (under age 18), Estimate...  \n",
      "4   Estimated annual rate (age 18+), Estimated ann...  \n",
      "5   Estimated annual rate (age 18+), Estimated ann...  \n",
      "6                                      Number per km2  \n",
      "7                                      Number per km2  \n",
      "8                                      Number per km2  \n",
      "9                               Estimated annual rate  \n",
      "10                              Estimated annual rate  \n",
      "11                    Estimated annual rate (age 30+)  \n",
      "12                                               Mean  \n",
      "13                                               Mean  \n",
      "14                       Annual average concentration  \n",
      "15                       Annual average concentration  \n",
      "16                                               Mean  \n",
      "17                              Estimated annual rate  \n"
     ]
    }
   ],
   "source": [
    "# Group by 'measure' and aggregate unique 'measure info' values\n",
    "grouped = df.groupby('Name')['Measure'].unique()\n",
    "\n",
    "# Convert the aggregated values into a comma-separated string\n",
    "grouped = grouped.apply(lambda x: ', '.join(x))\n",
    "\n",
    "# Convert the Series back to a DataFrame for saving to CSV\n",
    "unique_measures_df = grouped.reset_index()\n",
    "unique_measures_df.columns = ['measure', 'unique measure']\n",
    "\n",
    "# Print the information\n",
    "print(unique_measures_df)\n",
    "\n",
    "# Save to CSV\n",
    "unique_measures_df.to_csv('unique_measures.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230d58cd",
   "metadata": {},
   "source": [
    "The following code was used to pull the measurements and their unique measurement information. Again, the goal was to ensure that our data is uniform (i.e. there is only one measurement value per toxin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5b1e4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Measure              Unique Measure Info\n",
      "0          Annual average concentration                            Âµg/m3\n",
      "1                 Estimated annual rate  per 100,000 adults, per 100,000\n",
      "2       Estimated annual rate (age 18+)               per 100,000 adults\n",
      "3       Estimated annual rate (age 30+)               per 100,000 adults\n",
      "4  Estimated annual rate (under age 18)             per 100,000 children\n",
      "5                                  Mean                      ppb, mcg/m3\n",
      "6                        Number per km2                           number\n",
      "7                         million miles                          per km2\n"
     ]
    }
   ],
   "source": [
    "# Group by 'measure' and aggregate unique 'measure info' values\n",
    "grouped = df.groupby('Measure')['Measure Info'].unique()\n",
    "\n",
    "# Convert the aggregated values into a comma-separated string\n",
    "grouped = grouped.apply(lambda x: ', '.join(x))\n",
    "\n",
    "# Convert the Series back to a DataFrame for saving to CSV\n",
    "unique_measures_df = grouped.reset_index()\n",
    "unique_measures_df.columns = ['Measure', 'Unique Measure Info']\n",
    "\n",
    "# Print the information\n",
    "print(unique_measures_df)\n",
    "\n",
    "# Save to CSV\n",
    "unique_measures_df.to_csv('unique_measures_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a92982a",
   "metadata": {},
   "source": [
    "### Clean the time period data -- CURRENT BEING UNDONE\n",
    "\n",
    "The following code was used to pull the names of all unique time periods listed in the spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b611f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the column name you want to get unique values from\n",
    "column_name = 'Time Period'\n",
    "\n",
    "# Get unique values from the specified column\n",
    "unique_values = df[column_name].unique()\n",
    "\n",
    "# Convert the unique values into a new DataFrame\n",
    "unique_df = pd.DataFrame(unique_values, columns=[column_name])\n",
    "\n",
    "# Print the information\n",
    "print(unique_df)\n",
    "\n",
    "# Save the unique values to a new CSV file\n",
    "# unique_df.to_csv('unique_measure_values.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6d9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask to find rows with \"Time Period\" values matching \"Summer 20XX\"\n",
    "mask = df['Time Period'].str.contains(r'^Summer 20\\d{2}$', regex=True, na=False)\n",
    "\n",
    "# Extract those rows\n",
    "summer_rows = df[mask].copy()\n",
    "\n",
    "# Create new DataFrames for each summer month and update the \"Time Period\" column\n",
    "june_rows = summer_rows.copy()\n",
    "june_rows['Time Period'] = june_rows['Time Period'].str.replace(r'^Summer ', '06/01/')\n",
    "\n",
    "july_rows = summer_rows.copy()\n",
    "july_rows['Time Period'] = july_rows['Time Period'].str.replace(r'^Summer ', '07/01/')\n",
    "\n",
    "august_rows = summer_rows.copy()\n",
    "august_rows['Time Period'] = august_rows['Time Period'].str.replace(r'^Summer ', '08/01/')\n",
    "\n",
    "# Concatenate the original DataFrame with the new summer month rows (and exclude the original \"Summer 20XX\" rows)\n",
    "df = pd.concat([df[~mask], june_rows, july_rows, august_rows])\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv('Air_Quality_V7.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44203a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate monthly date range based on the given year\n",
    "def generate_dates(year):\n",
    "    if 2009 <= year <= 2014:\n",
    "        start_month = 12  # December of previous year\n",
    "        start_year = year - 1\n",
    "        end_month = 11  # November of current year\n",
    "        end_year = year\n",
    "    elif 2015 <= year <= 2020:\n",
    "        start_month = 1  # January of current year\n",
    "        start_year = year\n",
    "        end_month = 12  # December of current year\n",
    "        end_year = year\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "    dates = [f\"{year}-{month:02}-01\" for month in range(start_month, 13)] if start_year != end_year else []\n",
    "    dates.extend([f\"{end_year}-{month:02}-01\" for month in range(1, end_month+1)])\n",
    "    return dates\n",
    "\n",
    "# Extract rows matching \"Annual Average YYYY\" format and process them\n",
    "all_new_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    if \"Annual Average\" in row['Time Period']:\n",
    "        year = int(row['Time Period'].split()[-1])\n",
    "        dates = generate_dates(year)\n",
    "        \n",
    "        # Create duplicate rows for each date\n",
    "        for date in dates:\n",
    "            new_row = row.copy()\n",
    "            new_row['Time Period'] = date\n",
    "            all_new_rows.append(new_row)\n",
    "\n",
    "# Convert the list of new rows to a DataFrame\n",
    "new_rows_df = pd.DataFrame(all_new_rows)\n",
    "\n",
    "# Filter out the original \"Annual Average YYYY\" rows and concatenate with the new rows\n",
    "df = df[~df['Time Period'].str.contains(r\"^Annual Average \\d{4}$\", regex=True, na=False)]\n",
    "df = pd.concat([df, new_rows_df])\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('Air_Quality_V8.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e5b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('Air_Quality_V8.csv')\n",
    "\n",
    "# Function to generate winter month dates based on the given winter period\n",
    "def generate_winter_dates(period):\n",
    "    start_year = int(\"20\" + period.split('-')[0][-2:])\n",
    "    end_year = int(\"20\" + period.split('-')[1])\n",
    "    \n",
    "    return [f\"{start_year}-12-01\", f\"{end_year}-01-01\", f\"{end_year}-02-01\"]\n",
    "\n",
    "# Extract rows matching \"Winter 20XX-XX\" format and process them\n",
    "all_new_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    if \"Winter\" in row['Time Period']:\n",
    "        dates = generate_winter_dates(row['Time Period'].split()[-1])\n",
    "        \n",
    "        # Create duplicate rows for each winter date\n",
    "        for date in dates:\n",
    "            new_row = row.copy()\n",
    "            new_row['Time Period'] = date\n",
    "            all_new_rows.append(new_row)\n",
    "\n",
    "# Convert the list of new rows to a DataFrame\n",
    "new_rows_df = pd.DataFrame(all_new_rows)\n",
    "\n",
    "# Filter out the original \"Winter 20XX-XX\" rows and concatenate with the new rows\n",
    "df = df[~df['Time Period'].str.contains(r\"^Winter 20\\d{2}-\\d{2}$\", regex=True, na=False)]\n",
    "df = pd.concat([df, new_rows_df])\n",
    "\n",
    "# Save the updated DataFrame back to a CSV file\n",
    "df.to_csv('Air_Quality_V9.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8394eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('Air_Quality_V9.csv')\n",
    "\n",
    "# Function to generate month dates for an entire year\n",
    "def generate_month_dates(year):\n",
    "    return [f\"{year}-{month:02}-01\" for month in range(1, 13)]\n",
    "\n",
    "# Extract rows matching \"20XX\" format and process them\n",
    "all_new_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    if row['Time Period'].isdigit() and row['Time Period'].startswith(\"20\"):\n",
    "        dates = generate_month_dates(row['Time Period'])\n",
    "        \n",
    "        # Create duplicate rows for each month\n",
    "        for date in dates:\n",
    "            new_row = row.copy()\n",
    "            new_row['Time Period'] = date\n",
    "            all_new_rows.append(new_row)\n",
    "\n",
    "# Convert the list of new rows to a DataFrame\n",
    "new_rows_df = pd.DataFrame(all_new_rows)\n",
    "\n",
    "# Filter out the original \"20XX\" rows and concatenate with the new rows\n",
    "df = df[~df['Time Period'].str.match(r\"^20\\d{2}$\", na=False)]\n",
    "df = pd.concat([df, new_rows_df])\n",
    "\n",
    "# Save the updated DataFrame back to a CSV file\n",
    "df.to_csv('Air_Quality_V10.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bf4322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('Air_Quality_V10.csv')\n",
    "\n",
    "# Function to generate month dates for the entire range of years\n",
    "def generate_month_dates_for_range(start_year, end_year):\n",
    "    return [f\"{year}-{month:02}-01\" for year in range(start_year, end_year + 1) for month in range(1, 13)]\n",
    "\n",
    "# Extract rows matching \"YYYY-YYYY\" format and process them\n",
    "all_new_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    if \"-\" in row['Time Period'] and len(row['Time Period'].split(\"-\")) == 2:\n",
    "        try:\n",
    "            start_year, end_year = map(int, row['Time Period'].split(\"-\"))\n",
    "            dates = generate_month_dates_for_range(start_year, end_year)\n",
    "            \n",
    "            # Create duplicate rows for each month in the date range\n",
    "            for date in dates:\n",
    "                new_row = row.copy()\n",
    "                new_row['Time Period'] = date\n",
    "                all_new_rows.append(new_row)\n",
    "        except ValueError:\n",
    "            continue  # If conversion to int fails, it's not the format we're looking for, so continue\n",
    "\n",
    "# Convert the list of new rows to a DataFrame\n",
    "new_rows_df = pd.DataFrame(all_new_rows)\n",
    "\n",
    "# Filter out the original \"YYYY-YYYY\" rows and concatenate with the new rows\n",
    "df = df[~df['Time Period'].str.match(r\"^\\d{4}-\\d{4}$\", na=False)]\n",
    "df = pd.concat([df, new_rows_df])\n",
    "\n",
    "# Save the updated DataFrame back to a CSV file\n",
    "df.to_csv('Air_Quality_V11.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38935ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
